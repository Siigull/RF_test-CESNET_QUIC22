{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8162/8162 [00:07<00:00, 1032.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:04<00:00, 47.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1247/1247 [00:08<00:00, 148.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from cesnet_datazoo.datasets import CESNET_QUIC22\n",
    "from cesnet_datazoo.config import DatasetConfig, AppSelection, ValidationApproach\n",
    "\n",
    "dataset = CESNET_QUIC22(\"~/datasets/CESNET-QUIC22/\", size=\"XS\")\n",
    "\n",
    "common_params = {\n",
    "    \"dataset\": dataset,\n",
    "    \"apps_selection\": AppSelection.ALL_KNOWN,\n",
    "    \"train_period_name\": \"W-2022-44\",\n",
    "    \"val_approach\": ValidationApproach.SPLIT_FROM_TRAIN,\n",
    "    \"train_val_split_fraction\": 0.2,\n",
    "    \"use_packet_histograms\": True,\n",
    "}\n",
    "dataset_config = DatasetConfig(**common_params)\n",
    "dataset.set_dataset_config_and_initialize(dataset_config)\n",
    "train_dataframe = dataset.get_train_df(flatten_ppi=True)\n",
    "val_dataframe = dataset.get_val_df(flatten_ppi=True)\n",
    "test_dataframe = dataset.get_test_df(flatten_ppi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 416/7143 [32:20<8:42:58,  4.66s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 373\u001b[0m\n\u001b[1;32m    369\u001b[0m     q\u001b[38;5;241m.\u001b[39mclass_amount[y[i]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    371\u001b[0m state_key \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mupdate_state(State_key(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 373\u001b[0m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincreased_rd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrease_alpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 293\u001b[0m, in \u001b[0;36mQ.learn_parallel\u001b[0;34m(self, state_key, limit, increased_rd, decrease_alpha)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m/\u001b[39m increased_rd, \u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_value \u001b[38;5;241m-\u001b[39m decrease_alpha, \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m--> 293\u001b[0m state_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 235\u001b[0m, in \u001b[0;36mQ.learn_iter\u001b[0;34m(self, state_key, iter)\u001b[0m\n\u001b[1;32m    222\u001b[0m action_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(\n\u001b[1;32m    223\u001b[0m     state_key\u001b[38;5;241m=\u001b[39mstate_key,\n\u001b[1;32m    224\u001b[0m     next_action_list\u001b[38;5;241m=\u001b[39mnext_action_list\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(observe_reward_value,\n\u001b[1;32m    228\u001b[0m                             args \u001b[38;5;241m=\u001b[39m (action_key,\n\u001b[1;32m    229\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                     y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28miter\u001b[39m])\n\u001b[1;32m    233\u001b[0m                             )\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i] \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt]\n",
      "Cell \u001b[0;32mIn[21], line 235\u001b[0m, in \u001b[0;36mQ.learn_iter\u001b[0;34m(self, state_key, iter)\u001b[0m\n\u001b[1;32m    222\u001b[0m action_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(\n\u001b[1;32m    223\u001b[0m     state_key\u001b[38;5;241m=\u001b[39mstate_key,\n\u001b[1;32m    224\u001b[0m     next_action_list\u001b[38;5;241m=\u001b[39mnext_action_list\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(observe_reward_value,\n\u001b[1;32m    228\u001b[0m                             args \u001b[38;5;241m=\u001b[39m (action_key,\n\u001b[1;32m    229\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                     y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28miter\u001b[39m])\n\u001b[1;32m    233\u001b[0m                             )\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i] \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt]\n",
      "    \u001b[0;31m[... skipping similar frames: Q.learn_iter at line 235 (3 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 235\u001b[0m, in \u001b[0;36mQ.learn_iter\u001b[0;34m(self, state_key, iter)\u001b[0m\n\u001b[1;32m    222\u001b[0m action_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(\n\u001b[1;32m    223\u001b[0m     state_key\u001b[38;5;241m=\u001b[39mstate_key,\n\u001b[1;32m    224\u001b[0m     next_action_list\u001b[38;5;241m=\u001b[39mnext_action_list\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(observe_reward_value,\n\u001b[1;32m    228\u001b[0m                             args \u001b[38;5;241m=\u001b[39m (action_key,\n\u001b[1;32m    229\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_used[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                     y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28miter\u001b[39m])\n\u001b[1;32m    233\u001b[0m                             )\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_used[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i] \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt]\n",
      "Cell \u001b[0;32mIn[21], line 243\u001b[0m, in \u001b[0;36mQ.learn_iter\u001b[0;34m(self, state_key, iter)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_used[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i] \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt]\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 243\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m (clf, reward_value, cur_f1) \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from pyqlearning.q_learning import QLearning\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "# X_test = test_dataframe.drop(columns=\"APP\").to_numpy()[:10000]\n",
    "# y_test = test_dataframe[\"APP\"].to_numpy()[:10000]\n",
    "\n",
    "from datetime import datetime\n",
    "with open(\"out.txt\", \"a\") as f:\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    f.write(\"\\nstart(\" + time + \")\")\n",
    "\n",
    "def big_test(q, iters):\n",
    "    with open(\"out.txt\", \"a\") as f:\n",
    "        f.write(str(q.to_i) + \"\\n\")\n",
    "        \n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(q.X_used[:q.to_i], q.y_used[:q.to_i])\n",
    "        \n",
    "        predict_arr = clf.predict(X_big_test)\n",
    "        \n",
    "        f.write(f\"q_learning_acc: {accuracy_score(y_big_test, predict_arr):.4f}\" + \"\\n\")\n",
    "        \n",
    "        \n",
    "        clf = RandomForestClassifier()\n",
    "        indices = np.random.choice(base_samples_amount + iters, q.to_i, replace=False)\n",
    "\n",
    "        clf.fit(X[indices], y[indices])\n",
    "        \n",
    "        predict_arr = clf.predict(X_big_test)\n",
    "        \n",
    "        f.write(f\"random_learning_acc: {accuracy_score(y_big_test, predict_arr):.4f}\" + \"\\n\")\n",
    "        \n",
    "        \n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X[:base_samples_amount + iters], y[:base_samples_amount + iters])\n",
    "        \n",
    "        predict_arr = clf.predict(X_big_test)\n",
    "        \n",
    "        f.write(f\"total_learning_acc: {accuracy_score(y_big_test, predict_arr):.4f}\" + \"\\n\")\n",
    "        \n",
    "        q_df = q.q_df\n",
    "        q_df = q_df.sort_values(by=[\"q_value\"], ascending=False)\n",
    "        f.write(str(q_df.head()) + \"\\n\\n\")\n",
    "\n",
    "# same amount of samples from all classes\n",
    "def create_balanced_test_data():\n",
    "    grouped = test_dataframe.groupby('APP')\n",
    "\n",
    "    X_arr = np.ndarray(shape = (10100, X.shape[1]))\n",
    "    y_arr = np.ndarray(shape = (10100,))\n",
    "\n",
    "    for index, i in enumerate(grouped):\n",
    "        X_temp = i[1].drop(columns=\"APP\").to_numpy()\n",
    "        y_temp = i[1][\"APP\"].to_numpy()\n",
    "\n",
    "        X_arr[index*100:(index*100)+100] = X_temp[:100]\n",
    "        y_arr[index*100:(index*100)+100] = y_temp[:100]\n",
    "\n",
    "    return (X_arr, y_arr)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State_key:\n",
    "    percent_of_class : int  # 1e-2, 1e-1. 5e-1, 1, more than 1\n",
    "    predict_proba    : int  # 0-25, 25-50, 50-75, 75-100\n",
    "    correct_predict  : bool # True or False\n",
    "    percent_duration : int\n",
    "    # bytes_client     : int  # log3, 9 buckets\n",
    "    # bytes_server     : int  # log5, 7 buckets\n",
    "    duration         : int\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.percent_of_class + \\\n",
    "               self.predict_proba * 5 + \\\n",
    "               int(self.correct_predict) * 20 + \\\n",
    "               self.percent_duration * 40 + \\\n",
    "               self.duration * 320\n",
    "            #    self.bytes_client * 40 + \\\n",
    "            #    self.bytes_server * 360 + \\\n",
    "               \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, State_key) or self.__hash__() != other.__hash__():\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        # return self.percent_of_class == other.percent_of_class and \\\n",
    "        #        self.predict_proba == other.predict_proba and \\\n",
    "        #        self.percent_used == other.percent_used and \\\n",
    "        #        self.correct_predict == other.correct_predict\n",
    "        \n",
    "class Q(QLearning):\n",
    "    def get_clf_prediction(self, index):\n",
    "\n",
    "        proba = self.clf.predict_proba(X[index].reshape(1, -1))[0]\n",
    "        hit = (self.clf.predict(X[index].reshape(1, -1)) == y[index])[0]\n",
    "\n",
    "        return (proba, hit)\n",
    "    \n",
    "    def value_into_discrete(self, value, thresholds):\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if value < threshold:\n",
    "                return i\n",
    "        return len(thresholds)\n",
    "\n",
    "    def class_percent_into_discrete(self, percent):\n",
    "        return self.value_into_discrete(percent, self.CLASS_PERCENT_VALUES)\n",
    "\n",
    "    def predict_proba_into_discrete(self, proba):\n",
    "        return self.value_into_discrete(proba, self.PREDICT_PROBA_VALUES)\n",
    "\n",
    "    def percent_used_into_discrete(self, percent):\n",
    "        return self.value_into_discrete(percent, self.PERCENT_USED_VALUES)\n",
    "    \n",
    "    def duration_into_discrete(self, duration):\n",
    "        return self.value_into_discrete(duration, self.DURATION_VALUES)\n",
    "\n",
    "    def percent_duration_into_discrete(self, duration_percent):\n",
    "        return self.value_into_discrete(duration_percent, self.DURATION_PERCENT_VALUES)\n",
    "            \n",
    "    def client_bytes_into_discrete(self, nbytes):\n",
    "        return int(np.clip(int(log(nbytes, 3)) - 5, 1, 9) - 1)\n",
    "    \n",
    "    def server_bytes_into_discrete(self, nbytes):\n",
    "        return int(np.clip(int(log(nbytes, 5)) - 4, 0, 7))\n",
    "\n",
    "    def update_state(self, state_key, action_key, offset=0):\n",
    "        sample_index = self.base_i + self.t + 1 + offset\n",
    "\n",
    "        next_class = y[sample_index]\n",
    "\n",
    "        if action_key == 1 and offset == 0:\n",
    "            prev_duration = self.duration_into_discrete(self.X_used[self.to_i - 1][94])\n",
    "\n",
    "            self.duration_amount[prev_duration] += 1\n",
    "            self.class_amount[self.y_used[self.to_i - 1]] += 1\n",
    "            self.used += 1\n",
    "\n",
    "        class_percent = self.class_amount[next_class] / (self.used + self.base_samples)\n",
    "        \n",
    "        (proba, hit) = self.get_clf_prediction(sample_index)\n",
    "\n",
    "        res_index = np.where(self.clf.classes_ == next_class)\n",
    "        if len(res_index[0]):\n",
    "            proba = self.predict_proba_into_discrete(proba[res_index[0][0]])\n",
    "        else:\n",
    "            proba = 0\n",
    "\n",
    "        # client_bytes = self.client_bytes_into_discrete(X[sample_index][90])\n",
    "        # server_bytes = self.server_bytes_into_discrete(X[sample_index][91])\n",
    "\n",
    "        duration = self.duration_into_discrete(X[sample_index][94])\n",
    "        percent_duration = self.duration_amount[duration] / (self.used + self.base_samples)\n",
    "\n",
    "        return State_key(self.class_percent_into_discrete(class_percent), \n",
    "                         proba,\n",
    "                         hit,\n",
    "                         self.percent_duration_into_discrete(percent_duration),\n",
    "                         duration)\n",
    "\n",
    "    def initialize(self, cols, iters, already_used, nclasses):\n",
    "        self.q_count = defaultdict(int)\n",
    "\n",
    "        self.epsilon_greedy_rate = 0.9\n",
    "        self.alpha_value = 0.2\n",
    "        self.gamma_value = 0.9\n",
    "\n",
    "        self.clf = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "        self.CLASS_PERCENT_VALUES    = [0.0001, 0.01, 0.05, 0.1]\n",
    "        self.PREDICT_PROBA_VALUES    = [0.25, 0.50, 0.75]\n",
    "        self.DURATION_VALUES         = [0.1, 1, 29.9, 59.9, 89.9, 119.9, 299]\n",
    "        self.DURATION_PERCENT_VALUES = [0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "        self.pool_size = mp.cpu_count() - 1\n",
    "        self.pool = mp.Pool(self.pool_size)\n",
    "        self.used = 0\n",
    "        self.base_samples = already_used\n",
    "        self.base_i = already_used - 1\n",
    "        self.to_i = already_used\n",
    "\n",
    "        self.class_amount = defaultdict(int)\n",
    "        self.duration_amount = defaultdict(int)\n",
    "\n",
    "        self.X_used = np.ndarray(shape = (iters + already_used, cols))\n",
    "        self.y_used = np.ndarray(shape = (iters + already_used,))\n",
    "        self.last_f1 = 0\n",
    "    \n",
    "    def extract_possible_actions(self, state_key):\n",
    "        return list({0, 1})\n",
    "\n",
    "    def select_action(self, state_key, next_action_list):\n",
    "        epsilon_greedy_flag = bool(np.random.binomial(n=1, p=self.epsilon_greedy_rate))\n",
    "\n",
    "        if epsilon_greedy_flag is False:\n",
    "            action_key = random.choice(next_action_list)\n",
    "        else:\n",
    "            action_key = self.predict_next_action(state_key, next_action_list)\n",
    "\n",
    "        return action_key\n",
    "\n",
    "    def learn_iter(self, state_key, iter):\n",
    "        if iter >= self.pool_size:\n",
    "            return\n",
    "\n",
    "        state_key = self.update_state(state_key, 0, iter)\n",
    "\n",
    "        next_action_list = self.extract_possible_actions(state_key)\n",
    "        if len(next_action_list):\n",
    "            action_key = self.select_action(\n",
    "                state_key=state_key,\n",
    "                next_action_list=next_action_list\n",
    "            )\n",
    "            \n",
    "            result = self.pool.apply_async(observe_reward_value,\n",
    "                                        args = (action_key,\n",
    "                                                self.X_used[:self.to_i + 1], self.y_used[:self.to_i + 1],\n",
    "                                                self.last_f1,\n",
    "                                                X[self.base_i + self.t + iter],\n",
    "                                                y[self.base_i + self.t + iter])\n",
    "                                        )\n",
    "            \n",
    "            self.learn_iter(state_key, iter + 1)\n",
    "\n",
    "            if action_key == 1:\n",
    "                self.X_used[self.to_i] = X[self.base_i + self.t]\n",
    "                self.y_used[self.to_i] = y[self.base_i + self.t]\n",
    "\n",
    "                self.to_i += 1\n",
    "\n",
    "            result.wait()\n",
    "\n",
    "            (clf, reward_value, cur_f1) = result.get()\n",
    "            if iter == 0:\n",
    "                self.clf = clf\n",
    "            \n",
    "            self.save_r_df(state_key, reward_value)\n",
    "            self.last_f1 = cur_f1\n",
    "\n",
    "            if len(next_action_list):\n",
    "                # Max-Q-Value in next action time.\n",
    "                next_state_key = self.update_state(\n",
    "                    state_key=state_key,\n",
    "                    action_key=action_key\n",
    "                )\n",
    "\n",
    "                next_next_action_list = self.extract_possible_actions(next_state_key)\n",
    "                next_action_key = self.predict_next_action(next_state_key, next_next_action_list)\n",
    "                next_max_q = self.extract_q_df(next_state_key, next_action_key)\n",
    "\n",
    "                # Update Q-Value.\n",
    "                self.update_q(\n",
    "                    state_key=state_key,\n",
    "                    action_key=action_key,\n",
    "                    reward_value=reward_value,\n",
    "                    next_max_q=next_max_q\n",
    "                )\n",
    "                # Update State.\n",
    "                state_key = next_state_key\n",
    "\n",
    "            # Normalize.\n",
    "            self.normalize_q_value()\n",
    "            self.normalize_r_value()\n",
    "\n",
    "            # Epsode.\n",
    "            self.t += 1\n",
    "\n",
    "            return state_key\n",
    "\n",
    "    def learn_parallel(self, state_key, limit=1000, increased_rd = 1, decrease_alpha = 0):\n",
    "        self.t = 1\n",
    "        last_t = 1\n",
    "\n",
    "        for _ in tqdm(range(1, limit + 1, self.pool_size)):\n",
    "            if self.t - last_t > 100:\n",
    "                big_test(self, self.t)\n",
    "                last_t = self.t\n",
    "\n",
    "            self.epsilon_greedy_rate = min(self.t / increased_rd, 0.9)\n",
    "            self.alpha_value = max(self.alpha_value - decrease_alpha, 0.05)\n",
    "            state_key = self.learn_iter(state_key, 0)\n",
    "\n",
    "\n",
    "    def save_q_df(self, state_key, action_key, q_value):\n",
    "        if isinstance(q_value, float) is False:\n",
    "            raise TypeError(\"The type of q_value must be float.\")\n",
    "\n",
    "        new_q_df = pd.DataFrame([(state_key, action_key, q_value)], columns=[\"state_key\", \"action_key\", \"q_value\"])\n",
    "        \n",
    "        if q_value != 0.0:\n",
    "            self.q_count[(state_key, action_key)] += 1\n",
    "\n",
    "        if self.q_df is not None:\n",
    "            self.q_df = pd.concat([new_q_df, self.q_df])\n",
    "            self.q_df = self.q_df.drop_duplicates([\"state_key\", \"action_key\"])\n",
    "        else:\n",
    "            self.q_df = new_q_df\n",
    "\n",
    "    def observe_reward_value(self, state_key, action_key):\n",
    "        pass\n",
    "\n",
    "def test_acc(X_used, y_used, clf):\n",
    "    clf.fit(X_used, y_used)\n",
    "\n",
    "    predict_arr = clf.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, predict_arr, average=\"weighted\")\n",
    "\n",
    "def observe_reward_value(action_key, X_used, y_used, last_reward, X_s, y_s):\n",
    "    X_used[-1] = X_s\n",
    "    y_used[-1] = y_s\n",
    "\n",
    "    test_iters = 3\n",
    "    cur_f1 = 0\n",
    "    for i in range(test_iters):\n",
    "        clf = RandomForestClassifier(max_depth=10, n_jobs=1)\n",
    "        cur_f1 += test_acc(X_used, y_used, clf)\n",
    "\n",
    "    cur_f1 /= test_iters\n",
    "\n",
    "    reward = cur_f1 - last_reward\n",
    "\n",
    "    if action_key == 0:\n",
    "        reward = -reward\n",
    "\n",
    "    return (clf, reward, cur_f1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = train_dataframe.drop(columns=\"APP\").to_numpy()\n",
    "    y = train_dataframe[\"APP\"].to_numpy()\n",
    "\n",
    "    X_big_test = test_dataframe.drop(columns=\"APP\").to_numpy()[:100000]\n",
    "    y_big_test = test_dataframe[\"APP\"].to_numpy()[:100000]\n",
    "\n",
    "    (X_test, y_test) = create_balanced_test_data()\n",
    "\n",
    "    increased_rd = 500 # epsilon = min(self.t / increased_rd, 0.9)\n",
    "    decrease_alpha = 0.0001\n",
    "    iters = 50000\n",
    "    base_samples_amount = 400\n",
    "\n",
    "    nclasses = len(train_dataframe.groupby('APP'))\n",
    "\n",
    "    q = Q()\n",
    "    q.t = 1\n",
    "    q.initialize(X.shape[1], iters, base_samples_amount, nclasses)\n",
    "\n",
    "    q.X_used[:base_samples_amount] = X[:base_samples_amount]\n",
    "    q.y_used[:base_samples_amount] = y[:base_samples_amount]\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=10, n_jobs=1)\n",
    "    q.last_f1 = test_acc(q.X_used[:base_samples_amount], \n",
    "                        q.y_used[:base_samples_amount], clf)\n",
    "    q.clf = clf\n",
    "\n",
    "    for i in range(base_samples_amount):\n",
    "        q.class_amount[y[i]] += 1\n",
    "        q.duration_amount[q.duration_into_discrete(X[i][94])]\n",
    "\n",
    "    state_key = q.update_state(State_key(0, 0, 0, 0, 0), 0)\n",
    "\n",
    "    q.learn_parallel(state_key, iters, increased_rd, decrease_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cesnet_datazoo.datasets import CESNET_QUIC22\n",
    "from cesnet_datazoo.config import DatasetConfig, AppSelection, ValidationApproach\n",
    "\n",
    "dataset = CESNET_QUIC22(\"~/datasets/CESNET-QUIC22/\", size=\"XS\")\n",
    "\n",
    "common_params = {\n",
    "    \"dataset\": dataset,\n",
    "    \"apps_selection\": AppSelection.ALL_KNOWN,\n",
    "    \"train_period_name\": \"W-2022-44\",\n",
    "    \"val_approach\": ValidationApproach.SPLIT_FROM_TRAIN,\n",
    "    \"train_val_split_fraction\": 0.2,\n",
    "    \"use_packet_histograms\": True,\n",
    "}\n",
    "dataset_config = DatasetConfig(**common_params)\n",
    "dataset.set_dataset_config_and_initialize(dataset_config)\n",
    "train_dataframe = dataset.get_train_df(flatten_ppi=True)\n",
    "val_dataframe = dataset.get_val_df(flatten_ppi=True)\n",
    "test_dataframe = dataset.get_test_df(flatten_ppi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m---> 14\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataframe\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m train_dataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# X_test = test_dataframe.drop(columns=\"APP\").to_numpy()[:10000]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# y_test = test_dataframe[\"APP\"].to_numpy()[:10000]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# same amount of samples from all classes\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from pyqlearning.q_learning import QLearning\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "X = train_dataframe.drop(columns=\"APP\").to_numpy()\n",
    "y = train_dataframe[\"APP\"].to_numpy()\n",
    "\n",
    "# X_test = test_dataframe.drop(columns=\"APP\").to_numpy()[:10000]\n",
    "# y_test = test_dataframe[\"APP\"].to_numpy()[:10000]\n",
    "\n",
    "# same amount of samples from all classes\n",
    "def create_balanced_test_data():\n",
    "    grouped = test_dataframe.groupby('APP')\n",
    "\n",
    "    X_arr = np.ndarray(shape = (10100, X.shape[1]))\n",
    "    y_arr = np.ndarray(shape = (10100,))\n",
    "\n",
    "    for index, i in enumerate(grouped):\n",
    "        X_temp = i[1].drop(columns=\"APP\").to_numpy()\n",
    "        y_temp = i[1][\"APP\"].to_numpy()\n",
    "\n",
    "        X_arr[index*100:(index*100)+100] = X_temp[:100]\n",
    "        y_arr[index*100:(index*100)+100] = y_temp[:100]\n",
    "\n",
    "    return (X_arr, y_arr)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State_key:\n",
    "    percent_of_class : int  # 1e-2, 1e-1. 5e-1, 1, more than 1\n",
    "    predict_proba    : int  # 0-25, 25-50, 50-75, 75-100\n",
    "    correct_predict  : bool # True or False\n",
    "    percent_used     : int  # 1, 3, 5, 8, 13, 21\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.percent_of_class + \\\n",
    "               self.predict_proba * 5 + \\\n",
    "               int(self.correct_predict) * 10 + \\\n",
    "               self.percent_used * 40\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, State_key) or self.__hash__() != other.__hash__():\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        # return self.percent_of_class == other.percent_of_class and \\\n",
    "        #        self.predict_proba == other.predict_proba and \\\n",
    "        #        self.percent_used == other.percent_used and \\\n",
    "        #        self.correct_predict == other.correct_predict\n",
    "        \n",
    "\n",
    "class Q(QLearning):\n",
    "    def get_clf_prediction(self, index):\n",
    "        proba = self.clf.predict_proba(X[index].reshape(1, -1))[0]\n",
    "        hit = (self.clf.predict(X[index].reshape(1, -1)) == y[index])[0]\n",
    "\n",
    "        return (proba, hit)\n",
    "\n",
    "    def class_percent_into_discrete(self, percent):\n",
    "        for i in range(len(self.CLASS_PERCENT_VALUES)):\n",
    "            if percent < self.CLASS_PERCENT_VALUES[i]:\n",
    "                return i\n",
    "            \n",
    "        return len(self.CLASS_PERCENT_VALUES)\n",
    "    \n",
    "    def predict_proba_into_discrete(self, proba):\n",
    "        for i in range(len(self.PREDICT_PROBA_VALUES)):\n",
    "            if proba < self.PREDICT_PROBA_VALUES[i]:\n",
    "                return i\n",
    "            \n",
    "        return len(self.PREDICT_PROBA_VALUES)\n",
    "\n",
    "    def percent_used_into_discrete(self, percent):\n",
    "        for i in range(len(self.PERCENT_USED_VALUES)):\n",
    "            if percent < self.PERCENT_USED_VALUES[i]:\n",
    "                return i\n",
    "            \n",
    "        return len(self.PERCENT_USED_VALUES)\n",
    "\n",
    "    def update_state(self, state_key, action_key):\n",
    "        print(self.t, end = \" \")\n",
    "        print(state_key, end = \" \")\n",
    "        print(action_key)\n",
    "\n",
    "        next_class = y[self.base_i + self.t + 1]\n",
    "\n",
    "        if action_key == 1:\n",
    "            self.class_amount[self.y_used[self.to_i - 1]] += 1\n",
    "            self.used += 1\n",
    "\n",
    "        class_percent = self.class_amount[next_class] / (self.used + self.base_samples)\n",
    "        \n",
    "        (proba, hit) = self.get_clf_prediction(self.base_i + self.t + 1)\n",
    "\n",
    "        res_index = np.where(self.clf.classes_ == next_class)\n",
    "        if len(res_index[0]):\n",
    "            proba = self.predict_proba_into_discrete(proba[res_index[0][0]])\n",
    "        else:\n",
    "            proba = 0\n",
    "\n",
    "        percent_used = self.used / self.t\n",
    "\n",
    "        return State_key(self.class_percent_into_discrete(class_percent), \n",
    "                         proba,\n",
    "                         hit,\n",
    "                         self.percent_used_into_discrete(percent_used))\n",
    "\n",
    "    def initialize(self, cols, iters, already_used, nclasses):\n",
    "        self.epsilon_greedy_rate = 0.9\n",
    "        self.alpha_value = 0.1\n",
    "        self.gamma_value = 0.9\n",
    "\n",
    "        self.clf = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "        self.CLASS_PERCENT_VALUES = [0.0001, 0.01, 0.05, 0.1]\n",
    "        self.PREDICT_PROBA_VALUES = [0.25, 0.50, 0.75]\n",
    "        self.PERCENT_USED_VALUES  = [0.05, 0.08, 0.13, 0.21, 0.34, 0.89]\n",
    "\n",
    "        self.used = 0\n",
    "        self.base_samples = already_used\n",
    "        self.base_i = already_used - 1\n",
    "        self.to_i = already_used\n",
    "        self.class_amount = {}\n",
    "\n",
    "        for i in range(nclasses):\n",
    "            self.class_amount[i] = 0\n",
    "\n",
    "        self.X_used = np.ndarray(shape = (iters + already_used, cols))\n",
    "        self.y_used = np.ndarray(shape = (iters + already_used,))\n",
    "        self.last_f1 = 0\n",
    "    \n",
    "    def extract_possible_actions(self, state_key):\n",
    "        return list({0, 1})\n",
    "\n",
    "    def select_action(self, state_key, next_action_list):\n",
    "        epsilon_greedy_flag = bool(np.random.binomial(n=1, p=self.epsilon_greedy_rate))\n",
    "\n",
    "        if epsilon_greedy_flag is False:\n",
    "            action_key = random.choice(next_action_list)\n",
    "        else:\n",
    "            action_key = self.predict_next_action(state_key, next_action_list)\n",
    "\n",
    "        return action_key\n",
    "\n",
    "    def train_clf(self, clf):\n",
    "        clf.fit(self.X_used[:self.to_i], \n",
    "                self.y_used[:self.to_i])\n",
    "\n",
    "    def test_acc(self):\n",
    "        self.clf = RandomForestClassifier(max_depth=10)\n",
    "        self.train_clf(self.clf)\n",
    "\n",
    "        predict_arr = self.clf.predict(X_test)\n",
    "\n",
    "        return f1_score(y_test, predict_arr, average=\"weighted\")\n",
    "\n",
    "    def observe_reward_value(self, state_key, action_key):\n",
    "        # if action_key == 0:\n",
    "        #     return 0\n",
    "\n",
    "        self.X_used[self.to_i] = X[self.base_i + self.t]\n",
    "        self.y_used[self.to_i] = y[self.base_i + self.t]\n",
    "\n",
    "        self.to_i += 1\n",
    "\n",
    "        cur_f1 = self.test_acc()\n",
    "        reward = cur_f1 - self.last_f1\n",
    "\n",
    "        if action_key == 0:\n",
    "            self.to_i -= 1\n",
    "            reward = -reward\n",
    "        else:\n",
    "            self.last_f1 = cur_f1\n",
    "\n",
    "        # self.save_r_df(state_key, reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def learn(self, state_key, limit=1000, increased_rd = 1, decrease_alpha = 0):\n",
    "        self.t = 1\n",
    "\n",
    "        while self.t <= limit:\n",
    "            self.epsilon_greedy_rate = min(self.t / increased_rd, 0.9)\n",
    "            self.alpha_value = max(self.alpha_value - decrease_alpha, 0.05)\n",
    "            \n",
    "            next_action_list = self.extract_possible_actions(state_key)\n",
    "            if len(next_action_list):\n",
    "                action_key = self.select_action(\n",
    "                    state_key=state_key,\n",
    "                    next_action_list=next_action_list\n",
    "                )\n",
    "                reward_value = self.observe_reward_value(state_key, action_key)\n",
    "\n",
    "            if len(next_action_list):\n",
    "                # Max-Q-Value in next action time.\n",
    "                next_state_key = self.update_state(\n",
    "                    state_key=state_key,\n",
    "                    action_key=action_key\n",
    "                )\n",
    "\n",
    "                next_next_action_list = self.extract_possible_actions(next_state_key)\n",
    "                next_action_key = self.predict_next_action(next_state_key, next_next_action_list)\n",
    "                next_max_q = self.extract_q_df(next_state_key, next_action_key)\n",
    "\n",
    "                # Update Q-Value.\n",
    "                self.update_q(\n",
    "                    state_key=state_key,\n",
    "                    action_key=action_key,\n",
    "                    reward_value=reward_value,\n",
    "                    next_max_q=next_max_q\n",
    "                )\n",
    "                # Update State.\n",
    "                state_key = next_state_key\n",
    "\n",
    "            # Normalize.\n",
    "            self.normalize_q_value()\n",
    "            self.normalize_r_value()\n",
    "\n",
    "            # Vis.\n",
    "            self.visualize_learning_result(state_key)\n",
    "            # Check.\n",
    "            if self.check_the_end_flag(state_key) is True:\n",
    "                break\n",
    "\n",
    "            # Epsode.\n",
    "            self.t += 1\n",
    "    \n",
    "(X_test, y_test) = create_balanced_test_data()\n",
    "\n",
    "increased_rd = 100 # epsilon = min(self.t / increased_rd, 0.9)\n",
    "decrease_alpha = 0.001\n",
    "iters = 2000\n",
    "base_samples_amount = 400\n",
    "\n",
    "nclasses = len(train_dataframe.groupby('APP'))\n",
    "\n",
    "q = Q()\n",
    "q.t = 1\n",
    "q.initialize(X.shape[1], iters, base_samples_amount, nclasses)\n",
    "\n",
    "q.X_used[:base_samples_amount] = X[:base_samples_amount]\n",
    "q.y_used[:base_samples_amount] = y[:base_samples_amount]\n",
    "q.last_f1 = q.test_acc()\n",
    "\n",
    "for i in range(base_samples_amount):\n",
    "    q.class_amount[y[i]] += 1\n",
    "\n",
    "state_key = q.update_state(State_key(0, 0, 0, 0), 0)\n",
    "\n",
    "q.learn(state_key, iters, increased_rd, decrease_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
